{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "This example requires several Python libraries. Install boto3 to interact with AWS APIs, Pillow for image processing, matplotlib for visualization, GDAL for GeoTIFF file handling, requests for downloading aerial images, and pyproj for coordinate system conversions. Install these dependencies with pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c conda-forge gdal -y\n",
    "%pip install boto3 matplotlib pillow pyproj gdal requests --quiet\n",
    "import os\n",
    "os.environ['GTIFF_SRS_SOURCE'] = 'EPSG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from textwrap import dedent\n",
    "import time\n",
    "from pyproj import Transformer\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import display, clear_output\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import random\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Bedrock Runtime client and set the MODEL_ID variable. This example uses Amazon Nova Pro, which supports multimodal input and vision understanding. You can use other multimodal models, but you may need to adjust prompts and parameters for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.setup_default_session()\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=os.environ.get('AWS_REGION', 'us-east-1'))\n",
    "MODEL_ID = \"amazon.nova-pro-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model access with a simple request. If you encounter errors, verify that you have the required IAM permissions and that model access is enabled in the Amazon Bedrock console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": \"Hello\"}]}]\n",
    "    )\n",
    "    print(f\"✅ Model '{MODEL_ID}' is activated and available.\")\n",
    "\n",
    "except Exception as e:\n",
    "    if \"AccessDeniedException\" in str(e) or \"is not enabled\" in str(e):\n",
    "        print(f\"❌ Model '{MODEL_ID}' is not activated.\")\n",
    "        print(\"🔗 Activate it here: https://console.aws.amazon.com/bedrock/home?modelaccess#/modelaccess\")\n",
    "    else:\n",
    "        print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses high-resolution aerial imagery from SwissTopo's SWISSIMAGE 10 cm orthophoto mosaic. The imagery covers all of Switzerland with 10 cm ground resolution in plains and main valleys, and 25 cm resolution in the Alps. SwissTopo updates the imagery every 3 years. Download 1 square kilometer tiles from the [SwissTopo website](https://www.swisstopo.admin.ch/en/orthoimage-swissimage-10). The GeoTIFF format includes coordinate metadata that enables pixel-to-coordinate conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the link with the link to the image you want to inspect\n",
    "url = \"https://data.geo.admin.ch/ch.swisstopo.swissimage-dop10/swissimage-dop10_2022_2752-1227/swissimage-dop10_2022_2752-1227_0.1_2056.tif\"\n",
    "satellite_image_path = os.path.basename(url)\n",
    "with requests.get(url, stream=True, timeout=30) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(satellite_image_path, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load a GeoTIFF file and return the image data and metadata.\"\"\"\n",
    "dataset = gdal.Open(satellite_image_path)\n",
    "image_data = dataset.ReadAsArray()\n",
    "transform = dataset.GetGeoTransform()\n",
    "crs = dataset.GetProjection()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing Functions\n",
    "\n",
    "SwissTopo's high-resolution images work well for landmark detection but exceed Amazon Bedrock's request size limits. Split large images into 1000x1000 pixel chunks to stay within these limits. This function divides the 1km x 1km aerial image into 100 parts and returns each chunk with its offset coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(image_data, chunk_size=1000):\n",
    "    \"\"\"Split a large image into smaller chunks.\"\"\"\n",
    "    chunks = []\n",
    "    chunk_transforms = []\n",
    "\n",
    "    if len(image_data.shape) == 3:\n",
    "        bands, height, width = image_data.shape\n",
    "    else:\n",
    "        height, width = image_data.shape\n",
    "        bands = 1\n",
    "\n",
    "    num_chunks_y = (height + chunk_size - 1) // chunk_size\n",
    "    num_chunks_x = (width + chunk_size - 1) // chunk_size\n",
    "\n",
    "    for y in range(num_chunks_y):\n",
    "        for x in range(num_chunks_x):\n",
    "            y_start = y * chunk_size\n",
    "            y_end = min((y + 1) * chunk_size, height)\n",
    "            x_start = x * chunk_size\n",
    "            x_end = min((x + 1) * chunk_size, width)\n",
    "\n",
    "            if bands > 1:\n",
    "                chunk = image_data[:, y_start:y_end, x_start:x_end]\n",
    "            else:\n",
    "                chunk = image_data[y_start:y_end, x_start:x_end]\n",
    "\n",
    "            chunks.append(chunk)\n",
    "            chunk_transforms.append((x_start, y_start))\n",
    "\n",
    "    return chunks, chunk_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000 # Pixels - chunking required to not exceed request limit for large images\n",
    "chunks, chunk_transforms = split_image(image_data, chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert image arrays to JPEG bytes for the Converse API, which requires raw image data in bytes format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_bytes(image_array):\n",
    "    \"\"\"Convert numpy array to JPEG bytes.\"\"\"\n",
    "    # Convert to RGB if needed and ensure uint8 format\n",
    "    if image_array.shape[0] == 1:  # Single band\n",
    "        img = Image.fromarray(image_array[0].astype(np.uint8))\n",
    "    else:  # Multiple bands (assuming RGB)\n",
    "        # Rearrange from (bands, height, width) to (height, width, bands)\n",
    "        img = Image.fromarray(np.transpose(image_array, (1, 2, 0)).astype(np.uint8))\n",
    "\n",
    "    buffered = io.BytesIO()\n",
    "    img.save(buffered, format=\"JPEG\", quality=100, optimize=False)\n",
    "    return buffered.getvalue()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert pixel positions to real-world coordinates using GeoTIFF metadata and coordinate reference system information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_coords_to_geo(points, transform, crs):\n",
    "    \"\"\"Convert image coordinates to geographic coordinates using the transform.\"\"\"\n",
    "    coordinates = []\n",
    "\n",
    "    for point in points:\n",
    "        # GDAL transform: [x_origin, pixel_width, 0, y_origin, 0, -pixel_height]\n",
    "        projected_x = transform[0] + point[0] * transform[1] + point[1] * transform[2]\n",
    "        projected_y = transform[3] + point[0] * transform[4] + point[1] * transform[5]\n",
    "        transformer = Transformer.from_crs(crs, \"EPSG:4326\", always_xy=True)\n",
    "        lon, lat = transformer.transform(projected_x, projected_y)\n",
    "        coordinates.append((lon, lat))\n",
    "\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection Configuration\n",
    "\n",
    "Use function calling to ensure structured output from Amazon Nova Pro. Function calling guarantees consistent data format for downstream processing, eliminating the need to parse unstructured text responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for object detection\n",
    "OBJECT_CRITERIA = {\n",
    "    \"crosswalks\": \"Visible parallel striped markings (white or yellow paint) on paved roads or intersections with clear pedestrian crossing pattern. NOT lane markings, parking lines, building shadows, or road edges.\",\n",
    "    \"red_cars\": \"Vehicles that are clearly red in color, parked or moving on roads or parking areas. Must be distinguishable as cars (not trucks, motorcycles, or other vehicles).\"\n",
    "}\n",
    "\n",
    "# Define the generic object detection tool\n",
    "object_detection_tool = {\n",
    "    \"toolSpec\": {\n",
    "        \"name\": \"detect_objects\",\n",
    "        \"description\": \"ONLY call this function if you find the requested objects in the aerial image. Do not call if no objects are present.\",\n",
    "        \"inputSchema\": {\n",
    "            \"json\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Written description of what was found in the image\"\n",
    "                    },\n",
    "                    \"objects\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of detected objects matching the search criteria\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"name\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"Unique identifier for this object instance\"\n",
    "                                },\n",
    "                                \"object_type\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"Type of object detected (e.g., crosswalk, red_car, swimming_pool, parking_lot)\"\n",
    "                                },\n",
    "                                \"confidence\": {\n",
    "                                    \"type\": \"number\",\n",
    "                                    \"minimum\": 0,\n",
    "                                    \"maximum\": 1,\n",
    "                                    \"description\": \"Confidence score (0-1) that this is actually the requested object type\"\n",
    "                                },\n",
    "                                \"attributes\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"description\": \"Object-specific attributes (e.g., color, size, shape)\",\n",
    "                                    \"additionalProperties\": True\n",
    "                                },\n",
    "                                \"bounding_box\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"x_min\": {\"type\": \"integer\"},\n",
    "                                        \"y_min\": {\"type\": \"integer\"},\n",
    "                                        \"x_max\": {\"type\": \"integer\"},\n",
    "                                        \"y_max\": {\"type\": \"integer\"}\n",
    "                                    },\n",
    "                                    \"required\": [\"x_min\", \"y_min\", \"x_max\", \"y_max\"]\n",
    "                                }\n",
    "                            },\n",
    "                            \"required\": [\"name\", \"object_type\", \"confidence\", \"bounding_box\"]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"text\", \"objects\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_system_prompt(object_type):\n",
    "    criteria = OBJECT_CRITERIA.get(object_type, \"Objects matching the specified type and characteristics.\")\n",
    "    return dedent(f\"\"\"\\\n",
    "        You are a precise object detection system. Analyze aerial images to find ONLY genuine {object_type}.\n",
    "        \n",
    "        DETECTION CRITERIA: {criteria}\n",
    "        \n",
    "        ONLY call detect_objects function if you find actual {object_type} meeting the criteria.\n",
    "        If no {object_type} are found, do NOT call any function - just respond with text explaining what you see.\n",
    "        Be extremely conservative - false negatives are better than false positives.\n",
    "    \"\"\")\n",
    "\n",
    "def build_request(image_bytes, object_type=\"crosswalks\"):\n",
    "    system_prompt = create_system_prompt(object_type)\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"image\": {\"format\": \"jpeg\", \"source\": {\"bytes\": image_bytes}}},\n",
    "                    {\"text\": f\"Analyze this aerial image and detect any {object_type}. Use the detect_objects function to report your findings.\"}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"system\": [{\"text\": system_prompt}],\n",
    "        \"toolConfig\": {\n",
    "            \"tools\": [object_detection_tool],\n",
    "            \"toolChoice\": {\"auto\": {}}\n",
    "        },\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 2500,\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "    }\n",
    "\n",
    "def invoke_model(request_params, model_id):\n",
    "    max_retries = 3\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = bedrock_runtime.converse(modelId=model_id, **request_params)\n",
    "            print(\"✅ Inference completed successfully.\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            if (\n",
    "                    hasattr(e, 'response') and \n",
    "                    e.response.get(\"Error\", {}).get(\"Code\") == \"ThrottlingException\"\n",
    "                    and attempt < max_retries - 1\n",
    "                ):\n",
    "                wait_time = (2**attempt) + random.randint(0, 60)\n",
    "                print(\n",
    "                    f\"ThrottlingException, retrying in {wait_time}s (attempt {attempt + 1}/{max_retries})\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"❌ Model invocation error: {str(e)}\")\n",
    "                return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function calling provides structured output that matches your defined schema. The parse_response function extracts function call results directly, avoiding text parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(model_response):\n",
    "    if not model_response:\n",
    "        return None\n",
    "    try:\n",
    "        # Extract function call result from the response\n",
    "        content = model_response[\"output\"][\"message\"][\"content\"]\n",
    "        for item in content:\n",
    "            if \"toolUse\" in item:\n",
    "                tool_use = item[\"toolUse\"]\n",
    "                if tool_use[\"name\"] == \"detect_objects\":\n",
    "                    parsed = tool_use[\"input\"]\n",
    "                    print(\"✅ Parsed Output:\")\n",
    "                    print(json.dumps(parsed, indent=2))\n",
    "                    return parsed\n",
    "            elif \"text\" in item:\n",
    "                print(f\"Model response: {item['text']}\")\n",
    "        return None\n",
    "    except (KeyError, json.JSONDecodeError) as e:\n",
    "        print(f\"❌ Response parsing failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def draw_bounding_boxes(base64_img, objects, offset_x, offset_y):\n",
    "    img = Image.open(io.BytesIO(base64.b64decode(base64_img)))\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "    model_width = 1000\n",
    "    model_height = 1000\n",
    "\n",
    "    scale_x = img_width / model_width\n",
    "    scale_y = img_height / model_height\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(img)\n",
    "\n",
    "    for obj in objects:\n",
    "        box = obj[\"bounding_box\"]\n",
    "        point = convert_image_coords_to_geo([[box[\"x_min\"]+offset_x, box[\"y_min\"]+offset_y]], transform, crs)\n",
    "        obj[\"coordinates\"] = point[0]\n",
    "        if point:\n",
    "            x_min = box[\"x_min\"] * scale_x\n",
    "            y_min = box[\"y_min\"] * scale_y\n",
    "            x_max = box[\"x_max\"] * scale_x\n",
    "            y_max = box[\"y_max\"] * scale_y\n",
    "            label = f\"{obj['name']}\"\n",
    "            color = \"orange\"\n",
    "            rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                                linewidth=2, edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x_min, y_min - 10, label, color=color, fontsize=12, weight='bold')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_result(parsed, base64_image, offset_x, offset_y):\n",
    "\n",
    "    if parsed.get(\"objects\"):\n",
    "        print(\"🧾 Image with objects:\")\n",
    "        draw_bounding_boxes(base64_image, parsed[\"objects\"], offset_x, offset_y)\n",
    "\n",
    "        report_text = f\"\"\"{parsed['objects'][0]['name']}\n",
    "Description:\n",
    "{parsed['text']}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        for obj in parsed[\"objects\"]:\n",
    "            box = obj[\"bounding_box\"]\n",
    "            report_text += f\"\"\"\n",
    "- {obj['name']}:\n",
    "  Bounding Box: x_min={box['x_min']}, y_min={box['y_min']}, x_max={box['x_max']}, y_max={box['y_max']}\n",
    "  Coordinates: {obj[\"coordinates\"][1]} {obj[\"coordinates\"][0]}\n",
    "\"\"\"\n",
    "    else:\n",
    "\n",
    "        report_text = f\"\"\"No objects found\"\"\"\n",
    "    print(report_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point of Interest Detection\n",
    "\n",
    "Process the image chunks to detect points of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECT_TYPE = \"red_cars\"\n",
    "\n",
    "for i, (chunk, (offset_x, offset_y)) in enumerate(zip(chunks, chunk_transforms)):\n",
    "    img_bytes = encode_image_to_bytes(chunk)\n",
    "    request_params = build_request(img_bytes, OBJECT_TYPE)\n",
    "    model_response = invoke_model(request_params, MODEL_ID)\n",
    "    parsed_output = parse_response(model_response)\n",
    "    if parsed_output:\n",
    "        img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n",
    "        display_result(parsed_output, img_base64, offset_x, offset_y)\n",
    "    else:\n",
    "        print(f\"No {OBJECT_TYPE} found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search for something else change the `OBJECT_TYPE`. To search for a new object, add search criteria to `OBJECT_CRITERIA` and run the `for-loop` again with your new `OBJECT_TYPE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Delete the CloudFormation stack to remove the resources you deployed:\n",
    "\n",
    "```bash\n",
    "aws cloudformation delete-stack --stack-name GenerativePOIDetection\n",
    "```\n",
    "\n",
    "If you run the Jupyter notebook locally, no cleanup is needed as you have not provisioned any AWS resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "strands",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
